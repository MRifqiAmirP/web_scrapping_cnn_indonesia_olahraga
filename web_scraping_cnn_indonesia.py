# -*- coding: utf-8 -*-
"""Web Scraping CNN Indonesia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B2XoMHsIJ8Dv1gldLxI_YLjBzi4VIN5F
"""

import requests
from bs4 import BeautifulSoup as bs
import pandas as pd
from prettytable import PrettyTable
import google.generativeai as palm

url_cnn = requests.get('https://www.cnnindonesia.com/olahraga').text

url_soup = bs(url_cnn, "lxml")
berita_list = url_soup.find_all('span' > 'h2', {"class":"text-cnn_black_light dark:text-white mb-2 inline leading-normal text-xl group-hover:text-cnn_red"})
url_berita_list = url_soup.find_all('article' > 'a', {"class": "flex group items-center gap-4"})

try:
  judul_berita = []
  url_berita = []

  for url in url_berita_list:
    if "foto" in url.get('href'):
      continue
    url_berita.append(url.get('href'))
  print(len(url_berita))

  for berita in berita_list:
    if "FOTO" in berita.text:
      continue
    judul_berita.append(berita.text)
  print(len(judul_berita))

  df = pd.DataFrame({
    'No': range(1, len(judul_berita) + 1),
    'Judul Berita': judul_berita,
    'URL': url_berita
  })
except:
  judul_berita = []
  url_berita = []

  for url in url_berita_list[2:]:
    if "foto" in url.get('href'):
      continue
    url_berita.append(url.get('href'))
  print(len(url_berita))

  for berita in berita_list:
    if "FOTO" in berita.text:
      continue
    judul_berita.append(berita.text)
  print(len(judul_berita))

  df = pd.DataFrame({
    'No': range(1, len(judul_berita) + 1),
    'Judul Berita': judul_berita,
    'URL': url_berita
  })

pd.set_option('display.max_colwidth', None)
# for country in country_list:
#     print(country.text)

table = PrettyTable()
table.field_names = ["No","Judul Berita", "URL"]

for index, row in df.iterrows():
    table.add_row([row['No'], row['Judul Berita'], row['URL']])

table.align["Judul Berita"] = "l"
table.align["URL"] = "l"

print(table)

inputBerita = input("Masukkan nomor berita: ")
print("Berita yang dipilih: " + judul_berita[int(inputBerita)-1])
berita = requests.get(url_berita[int(inputBerita)-1]).text

baca_berita_soup = bs(berita, "lxml")
baca_berita = baca_berita_soup.find('div', {'class':'detail-text text-cnn_black text-sm grow min-w-0'})
baca_beritaa = baca_berita.find_all('p')
print(baca_beritaa)
print('\n')

for baca in baca_beritaa:
  if baca.get('class') == ['para_caption'] or baca.find('a', {"class":"embed videocnn"}):
        continue
  text = baca.get_text()
  print(text)

text_join = ""
index = 0

for baca in baca_beritaa:
  index += 1
  if baca.get('class') == ['para_caption'] or baca.find('a', {"class":"embed videocnn"}):
        continue
  text_join += baca.text
  text_join += " \n"

print(text_join)

# !pip install google-generativeai
# !pip install --upgrade google-generativeai

palm.configure(api_key="AIzaSyBwutDVoYB2aZbLBYfUvxHna40fsAsb9BE")
model = palm.GenerativeModel("gemini-1.5-flash")
response = model.generate_content(text_join)
print(response.text)

# df.to_csv('cnn_news.csv', encoding='utf-8-sig')
# files.download('cnn_news.csv')